{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Classification\n",
    "\n",
    "1. Load dataset: X (features), y (labels)\n",
    "2. Initialize model (with random parameters: w, b)\n",
    "3. Evaluate the model with a metric (e.g. BCE).\n",
    "4. Calculate gradient of loss.\n",
    "5. Update parameters a small step on the directions descending the gradient of loss.\n",
    "6. Repeat 3 to 5 until converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "features_raw, labels_raw = load_breast_cancer(return_X_y=True)\n",
    "print(features_raw.shape, labels_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Pre-Process Dataset\n",
    "1. Rescale features within range $[0, 1]$\n",
    "2. Reshape arrays to 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features_raw / features_raw.max(axis=0)\n",
    "labels_train = labels_raw.reshape(-1, 1)\n",
    "\n",
    "# Sanity check\n",
    "print(f\"Training features shape: {features_train.shape}, Training labels shape: {labels_train.shape}\")\n",
    "print(f\"Samples of training features:\\n{features_train[:3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Initialize Model\n",
    "$$\\mathbf{\\hat{y}} = \\sigma(\\mathbf{X} \\cdot \\mathbf{w}^T + b) = \\sigma(\\mathbf{Z})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define linear function\n",
    "def linear(in_features, weights, bias):\n",
    "    return in_features @ weights.T + bias\n",
    "\n",
    "# Define ReLU function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Redefine forward pass. Intermediate result, Z, needs to be tracked \n",
    "def forward(in_features, weights, bias):\n",
    "    out_features = linear(in_features, weights, bias)\n",
    "    preds = sigmoid(out_features)\n",
    "    return preds\n",
    "\n",
    "# Sanity check\n",
    "np.random.seed(123)\n",
    "w_dummy = np.random.normal(loc=0, scale=0.1, size=(1, features_train.shape[1]))  # shape: (1, 30)\n",
    "b_dummy = np.random.normal(0, 0.1)  # shape: scalar\n",
    "preds_dummy = forward(features_train, w_dummy, b_dummy)\n",
    "print(f\"Samples of dummy predictions: {preds_dummy[-3:]}\")\n",
    "predclas_dummy = preds_dummy > 0.5\n",
    "print(f\"Sampels of predicted classes (dummy): {predclas_dummy[-3:]}\")\n",
    "accuracy = np.sum(predclas_dummy==labels_train) / labels_train.shape[0]\n",
    "print(f\"Dummy classification accuracy: {accuracy*100:.2f}%\")\n",
    "print(\"Practice computing other metrics, e.g., precision, recall, F1-score, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Evaluate Model with Binary Cross-Entropy Loss\n",
    "$\\mathcal{L}(\\hat{\\mathbf{y}}, \\mathbf{y}) = \\frac{1}{M} \\sum_{i=1}^{M} -{}^{(i)}y \\ln {}^{(i)}\\hat{y} - (1 - {}^{(i)}y) \\ln (1 - {}^{(i)}\\hat{y}) = \\overline{-\\mathbf{y} \\ln \\hat{\\mathbf{y}} - (1 - \\mathbf{y}) \\ln (1 - \\hat{\\mathbf{y}})}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(preds, labels):\n",
    "    error = -labels * np.log(preds) - (1 - labels) * np.log(1 - preds)\n",
    "    return np.mean(error)\n",
    "\n",
    "# Sanity check\n",
    "loss_dummy = bce_loss(preds_dummy, labels_train)\n",
    "print(f\"Binary cross entropy loss of the dummy model: {loss_dummy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Gradient Descent Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(in_features, preds, labels):\n",
    "    dw = 1 / labels.shape[0] * (preds - labels).T @ in_features\n",
    "    db = np.mean(preds - labels)\n",
    "    return dw, db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Update Parameters following Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameters: w = [[ 3.14271995e-05 -1.32626546e-04  1.41729905e-04  8.07236535e-05\n",
      "   4.54900806e-06 -2.33092061e-05 -1.19830114e-04  1.99524074e-05\n",
      "   4.68439119e-05 -8.31154984e-05  1.16220405e-04 -1.09720305e-04\n",
      "  -2.12310035e-04  1.03972709e-04 -4.03366038e-05 -1.26029585e-05\n",
      "  -8.37516723e-05 -1.60596276e-04  1.25523737e-04 -6.88868984e-05\n",
      "   1.66095249e-04  8.07308186e-05 -3.14758147e-05 -1.08590240e-04\n",
      "  -7.32461987e-05 -1.21252313e-04  2.08711336e-04  1.64441230e-05\n",
      "   1.15020554e-04 -1.26735205e-04]], b = 1.8103512959700387e-05\n",
      "loss @ 1 iteration: 0.6931542915781066\n",
      "loss @ 2 iteration: 0.6634570561745241\n",
      "loss @ 3 iteration: 0.6439817256365713\n",
      "loss @ 4 iteration: 0.6257564039680796\n",
      "loss @ 5 iteration: 0.6086844376984637\n",
      "loss @ 6 iteration: 0.5926852867522654\n",
      "loss @ 7 iteration: 0.5776815336511125\n",
      "loss @ 8 iteration: 0.5635996014037183\n",
      "loss @ 9 iteration: 0.5503702148430745\n",
      "loss @ 10 iteration: 0.5379286449197452\n",
      "loss @ 11 iteration: 0.526214778677398\n",
      "loss @ 12 iteration: 0.5151730571114712\n",
      "loss @ 13 iteration: 0.5047523186696213\n",
      "loss @ 14 iteration: 0.4949055799826076\n",
      "loss @ 15 iteration: 0.4855897788876071\n",
      "loss @ 16 iteration: 0.47676549874803026\n",
      "loss @ 17 iteration: 0.4683966878844018\n",
      "loss @ 18 iteration: 0.46045038372684843\n",
      "loss @ 19 iteration: 0.4528964480355111\n",
      "loss @ 20 iteration: 0.4457073170873779\n",
      "loss @ 21 iteration: 0.43885776894844214\n",
      "loss @ 22 iteration: 0.43232470869503625\n",
      "loss @ 23 iteration: 0.42608697159195447\n",
      "loss @ 24 iteration: 0.4201251436742693\n",
      "loss @ 25 iteration: 0.41442139883306106\n",
      "loss @ 26 iteration: 0.40895935131029293\n",
      "loss @ 27 iteration: 0.40372392241814736\n",
      "loss @ 28 iteration: 0.3987012202790958\n",
      "loss @ 29 iteration: 0.393878431410117\n",
      "loss @ 30 iteration: 0.3892437230302913\n",
      "loss @ 31 iteration: 0.3847861550432178\n",
      "loss @ 32 iteration: 0.3804956007259414\n",
      "loss @ 33 iteration: 0.37636267523869293\n",
      "loss @ 34 iteration: 0.37237867115108164\n",
      "loss @ 35 iteration: 0.3685355002581532\n",
      "loss @ 36 iteration: 0.36482564103260734\n",
      "loss @ 37 iteration: 0.3612420911268122\n",
      "loss @ 38 iteration: 0.3577783243997974\n",
      "loss @ 39 iteration: 0.3544282520002454\n",
      "loss @ 40 iteration: 0.3511861870868335\n",
      "loss @ 41 iteration: 0.3480468128124671\n",
      "loss @ 42 iteration: 0.34500515323936437\n",
      "loss @ 43 iteration: 0.34205654688802717\n",
      "loss @ 44 iteration: 0.3391966226552567\n",
      "loss @ 45 iteration: 0.3364212778649547\n",
      "loss @ 46 iteration: 0.3337266582408491\n",
      "loss @ 47 iteration: 0.33110913961284183\n",
      "loss @ 48 iteration: 0.32856531118871374\n",
      "loss @ 49 iteration: 0.32609196024070936\n",
      "loss @ 50 iteration: 0.32368605807232426\n",
      "loss @ 51 iteration: 0.32134474714465716\n",
      "loss @ 52 iteration: 0.31906532925416137\n",
      "loss @ 53 iteration: 0.3168452546647276\n",
      "loss @ 54 iteration: 0.3146821121069034\n",
      "loss @ 55 iteration: 0.3125736195658432\n",
      "loss @ 56 iteration: 0.31051761578742276\n",
      "loss @ 57 iteration: 0.3085120524389365\n",
      "loss @ 58 iteration: 0.3065549868670361\n",
      "loss @ 59 iteration: 0.30464457540114304\n",
      "loss @ 60 iteration: 0.30277906715555125\n",
      "loss @ 61 iteration: 0.3009567982878996\n",
      "loss @ 62 iteration: 0.299176186675691\n",
      "loss @ 63 iteration: 0.2974357269761197\n",
      "loss @ 64 iteration: 0.2957339860376901\n",
      "loss @ 65 iteration: 0.29406959863499854\n",
      "loss @ 66 iteration: 0.2924412635006552\n",
      "loss @ 67 iteration: 0.29084773963066346\n",
      "loss @ 68 iteration: 0.28928784284168496\n",
      "loss @ 69 iteration: 0.28776044256052447\n",
      "loss @ 70 iteration: 0.28626445882788637\n",
      "loss @ 71 iteration: 0.2847988595000099\n",
      "loss @ 72 iteration: 0.28336265763319635\n",
      "loss @ 73 iteration: 0.28195490903751314\n",
      "loss @ 74 iteration: 0.280574709987116\n",
      "loss @ 75 iteration: 0.2792211950756745\n",
      "loss @ 76 iteration: 0.2778935352063404\n",
      "loss @ 77 iteration: 0.2765909357065579\n",
      "loss @ 78 iteration: 0.2753126345588048\n",
      "loss @ 79 iteration: 0.27405790073906644\n",
      "loss @ 80 iteration: 0.2728260326554978\n",
      "loss @ 81 iteration: 0.27161635668032286\n",
      "loss @ 82 iteration: 0.2704282257685633\n",
      "loss @ 83 iteration: 0.2692610181576857\n",
      "loss @ 84 iteration: 0.2681141361427082\n",
      "loss @ 85 iteration: 0.26698700492172184\n",
      "loss @ 86 iteration: 0.26587907150716644\n",
      "loss @ 87 iteration: 0.2647898036985415\n",
      "loss @ 88 iteration: 0.26371868911255886\n",
      "loss @ 89 iteration: 0.26266523426703375\n",
      "loss @ 90 iteration: 0.2616289637150804\n",
      "loss @ 91 iteration: 0.2606094192264258\n",
      "loss @ 92 iteration: 0.25960615901288486\n",
      "loss @ 93 iteration: 0.2586187569952477\n",
      "loss @ 94 iteration: 0.2576468021090228\n",
      "loss @ 95 iteration: 0.25668989764666095\n",
      "loss @ 96 iteration: 0.25574766063404475\n",
      "loss @ 97 iteration: 0.2548197212391851\n",
      "loss @ 98 iteration: 0.25390572221120034\n",
      "loss @ 99 iteration: 0.25300531834778983\n",
      "loss @ 100 iteration: 0.2521181759895275\n",
      "loss @ 101 iteration: 0.25124397253941644\n",
      "loss @ 102 iteration: 0.2503823960062457\n",
      "loss @ 103 iteration: 0.2495331445703857\n",
      "loss @ 104 iteration: 0.2486959261707495\n",
      "loss @ 105 iteration: 0.2478704581117253\n",
      "loss @ 106 iteration: 0.24705646668896486\n",
      "loss @ 107 iteration: 0.2462536868329811\n",
      "loss @ 108 iteration: 0.24546186176957538\n",
      "loss @ 109 iteration: 0.24468074269617426\n",
      "loss @ 110 iteration: 0.2439100884732147\n",
      "loss @ 111 iteration: 0.24314966532976665\n",
      "loss @ 112 iteration: 0.2423992465826347\n",
      "loss @ 113 iteration: 0.2416586123682234\n",
      "loss @ 114 iteration: 0.24092754938649483\n",
      "loss @ 115 iteration: 0.2402058506563893\n",
      "loss @ 116 iteration: 0.23949331528211165\n",
      "loss @ 117 iteration: 0.2387897482297276\n",
      "loss @ 118 iteration: 0.23809496011354137\n",
      "loss @ 119 iteration: 0.2374087669917597\n",
      "loss @ 120 iteration: 0.23673099017097474\n",
      "loss @ 121 iteration: 0.2360614560190241\n",
      "loss @ 122 iteration: 0.23539999578581378\n",
      "loss @ 123 iteration: 0.23474644543170983\n",
      "loss @ 124 iteration: 0.23410064546312997\n",
      "loss @ 125 iteration: 0.23346244077498407\n",
      "loss @ 126 iteration: 0.23283168049963296\n",
      "loss @ 127 iteration: 0.23220821786205398\n",
      "loss @ 128 iteration: 0.2315919100409167\n",
      "loss @ 129 iteration: 0.2309826180352895\n",
      "loss @ 130 iteration: 0.23038020653671334\n",
      "loss @ 131 iteration: 0.2297845438063907\n",
      "loss @ 132 iteration: 0.22919550155725393\n",
      "loss @ 133 iteration: 0.22861295484068758\n",
      "loss @ 134 iteration: 0.22803678193769178\n",
      "loss @ 135 iteration: 0.22746686425428544\n",
      "loss @ 136 iteration: 0.22690308622095665\n",
      "loss @ 137 iteration: 0.2263453351959792\n",
      "loss @ 138 iteration: 0.22579350137242363\n",
      "loss @ 139 iteration: 0.22524747768869763\n",
      "loss @ 140 iteration: 0.22470715974246122\n",
      "loss @ 141 iteration: 0.22417244570776926\n",
      "loss @ 142 iteration: 0.22364323625530022\n",
      "loss @ 143 iteration: 0.22311943447553825\n",
      "loss @ 144 iteration: 0.2226009458047819\n",
      "loss @ 145 iteration: 0.2220876779538585\n",
      "loss @ 146 iteration: 0.221579540839429\n",
      "loss @ 147 iteration: 0.2210764465177753\n",
      "loss @ 148 iteration: 0.22057830912096466\n",
      "loss @ 149 iteration: 0.22008504479529248\n",
      "loss @ 150 iteration: 0.2195965716419098\n",
      "loss @ 151 iteration: 0.21911280965954455\n",
      "loss @ 152 iteration: 0.21863368068923145\n",
      "loss @ 153 iteration: 0.21815910836096825\n",
      "loss @ 154 iteration: 0.21768901804222138\n",
      "loss @ 155 iteration: 0.21722333678820535\n",
      "loss @ 156 iteration: 0.21676199329386567\n",
      "loss @ 157 iteration: 0.21630491784749747\n",
      "loss @ 158 iteration: 0.21585204228593508\n",
      "loss @ 159 iteration: 0.21540329995125054\n",
      "loss @ 160 iteration: 0.21495862564890234\n",
      "loss @ 161 iteration: 0.21451795560727802\n",
      "loss @ 162 iteration: 0.21408122743857622\n",
      "loss @ 163 iteration: 0.2136483801009774\n",
      "loss @ 164 iteration: 0.21321935386205326\n",
      "loss @ 165 iteration: 0.21279409026336837\n",
      "loss @ 166 iteration: 0.2123725320862282\n",
      "loss @ 167 iteration: 0.21195462331853085\n",
      "loss @ 168 iteration: 0.21154030912268124\n",
      "loss @ 169 iteration: 0.21112953580452803\n",
      "loss @ 170 iteration: 0.21072225078328527\n",
      "loss @ 171 iteration: 0.21031840256240258\n",
      "loss @ 172 iteration: 0.20991794070134956\n",
      "loss @ 173 iteration: 0.20952081578828022\n",
      "loss @ 174 iteration: 0.20912697941354633\n",
      "loss @ 175 iteration: 0.20873638414402856\n",
      "loss @ 176 iteration: 0.2083489834982564\n",
      "loss @ 177 iteration: 0.20796473192228834\n",
      "loss @ 178 iteration: 0.20758358476632569\n",
      "loss @ 179 iteration: 0.20720549826203363\n",
      "loss @ 180 iteration: 0.20683042950054528\n",
      "loss @ 181 iteration: 0.20645833641112427\n",
      "loss @ 182 iteration: 0.20608917774046287\n",
      "loss @ 183 iteration: 0.20572291303259468\n",
      "loss @ 184 iteration: 0.20535950260939906\n",
      "loss @ 185 iteration: 0.20499890755167843\n",
      "loss @ 186 iteration: 0.20464108968078817\n",
      "loss @ 187 iteration: 0.20428601154080034\n",
      "loss @ 188 iteration: 0.20393363638118364\n",
      "loss @ 189 iteration: 0.20358392813998188\n",
      "loss @ 190 iteration: 0.203236851427474\n",
      "loss @ 191 iteration: 0.20289237151030043\n",
      "loss @ 192 iteration: 0.20255045429603935\n",
      "loss @ 193 iteration: 0.20221106631821895\n",
      "loss @ 194 iteration: 0.20187417472175048\n",
      "loss @ 195 iteration: 0.20153974724876908\n",
      "loss @ 196 iteration: 0.2012077522248685\n",
      "loss @ 197 iteration: 0.20087815854571742\n",
      "loss @ 198 iteration: 0.2005509356640449\n",
      "loss @ 199 iteration: 0.20022605357698306\n",
      "loss @ 200 iteration: 0.19990348281375556\n",
      "loss @ 201 iteration: 0.19958319442370104\n",
      "loss @ 202 iteration: 0.1992651599646209\n",
      "loss @ 203 iteration: 0.19894935149144075\n",
      "loss @ 204 iteration: 0.19863574154517666\n",
      "loss @ 205 iteration: 0.19832430314219554\n",
      "loss @ 206 iteration: 0.19801500976376146\n",
      "loss @ 207 iteration: 0.19770783534585828\n",
      "loss @ 208 iteration: 0.197402754269281\n",
      "loss @ 209 iteration: 0.19709974134998642\n",
      "loss @ 210 iteration: 0.19679877182969627\n",
      "loss @ 211 iteration: 0.19649982136674457\n",
      "loss @ 212 iteration: 0.19620286602716178\n",
      "loss @ 213 iteration: 0.19590788227598893\n",
      "loss @ 214 iteration: 0.19561484696881468\n",
      "loss @ 215 iteration: 0.19532373734352831\n",
      "loss @ 216 iteration: 0.195034531012283\n",
      "loss @ 217 iteration: 0.1947472059536622\n",
      "loss @ 218 iteration: 0.19446174050504408\n",
      "loss @ 219 iteration: 0.19417811335515736\n",
      "loss @ 220 iteration: 0.1938963035368233\n",
      "loss @ 221 iteration: 0.19361629041987888\n",
      "loss @ 222 iteration: 0.19333805370427498\n",
      "loss @ 223 iteration: 0.19306157341334532\n",
      "loss @ 224 iteration: 0.19278682988724058\n",
      "loss @ 225 iteration: 0.19251380377652372\n",
      "loss @ 226 iteration: 0.1922424760359212\n",
      "loss @ 227 iteration: 0.19197282791822592\n",
      "loss @ 228 iteration: 0.191704840968348\n",
      "loss @ 229 iteration: 0.19143849701750812\n",
      "loss @ 230 iteration: 0.19117377817757125\n",
      "loss @ 231 iteration: 0.19091066683551447\n",
      "loss @ 232 iteration: 0.19064914564802743\n",
      "loss @ 233 iteration: 0.19038919753624028\n",
      "loss @ 234 iteration: 0.19013080568057597\n",
      "loss @ 235 iteration: 0.18987395351572414\n",
      "loss @ 236 iteration: 0.18961862472573168\n",
      "loss @ 237 iteration: 0.1893648032392087\n",
      "loss @ 238 iteration: 0.18911247322464547\n",
      "loss @ 239 iteration: 0.18886161908583757\n",
      "loss @ 240 iteration: 0.18861222545741668\n",
      "loss @ 241 iteration: 0.188364277200484\n",
      "loss @ 242 iteration: 0.188117759398343\n",
      "loss @ 243 iteration: 0.18787265735233027\n",
      "loss @ 244 iteration: 0.1876289565777399\n",
      "loss @ 245 iteration: 0.18738664279984074\n",
      "loss @ 246 iteration: 0.18714570194998295\n",
      "loss @ 247 iteration: 0.18690612016179187\n",
      "loss @ 248 iteration: 0.18666788376744703\n",
      "loss @ 249 iteration: 0.186430979294044\n",
      "loss @ 250 iteration: 0.1861953934600365\n",
      "loss @ 251 iteration: 0.18596111317175765\n",
      "loss @ 252 iteration: 0.18572812552001722\n",
      "loss @ 253 iteration: 0.18549641777677384\n",
      "loss @ 254 iteration: 0.18526597739187978\n",
      "loss @ 255 iteration: 0.18503679198989617\n",
      "loss @ 256 iteration: 0.18480884936697792\n",
      "loss @ 257 iteration: 0.1845821374878253\n",
      "loss @ 258 iteration: 0.18435664448270125\n",
      "loss @ 259 iteration: 0.1841323586445129\n",
      "loss @ 260 iteration: 0.18390926842595506\n",
      "loss @ 261 iteration: 0.18368736243671438\n",
      "loss @ 262 iteration: 0.18346662944073336\n",
      "loss @ 263 iteration: 0.18324705835353144\n",
      "loss @ 264 iteration: 0.18302863823958296\n",
      "loss @ 265 iteration: 0.18281135830974984\n",
      "loss @ 266 iteration: 0.18259520791876785\n",
      "loss @ 267 iteration: 0.1823801765627854\n",
      "loss @ 268 iteration: 0.18216625387695334\n",
      "loss @ 269 iteration: 0.18195342963306427\n",
      "loss @ 270 iteration: 0.18174169373724097\n",
      "loss @ 271 iteration: 0.18153103622767167\n",
      "loss @ 272 iteration: 0.18132144727239205\n",
      "loss @ 273 iteration: 0.18111291716711256\n",
      "loss @ 274 iteration: 0.18090543633308892\n",
      "loss @ 275 iteration: 0.18069899531503703\n",
      "loss @ 276 iteration: 0.18049358477908892\n",
      "loss @ 277 iteration: 0.18028919551079\n",
      "loss @ 278 iteration: 0.1800858184131368\n",
      "loss @ 279 iteration: 0.17988344450465316\n",
      "loss @ 280 iteration: 0.1796820649175052\n",
      "loss @ 281 iteration: 0.17948167089565353\n",
      "loss @ 282 iteration: 0.17928225379304175\n",
      "loss @ 283 iteration: 0.17908380507182067\n",
      "loss @ 284 iteration: 0.1788863163006077\n",
      "loss @ 285 iteration: 0.17868977915277942\n",
      "loss @ 286 iteration: 0.17849418540479844\n",
      "loss @ 287 iteration: 0.17829952693457185\n",
      "loss @ 288 iteration: 0.17810579571984186\n",
      "loss @ 289 iteration: 0.1779129838366076\n",
      "loss @ 290 iteration: 0.1777210834575766\n",
      "loss @ 291 iteration: 0.17753008685064667\n",
      "loss @ 292 iteration: 0.1773399863774164\n",
      "loss @ 293 iteration: 0.1771507744917237\n",
      "loss @ 294 iteration: 0.1769624437382128\n",
      "loss @ 295 iteration: 0.17677498675092745\n",
      "loss @ 296 iteration: 0.17658839625193132\n",
      "loss @ 297 iteration: 0.17640266504995378\n",
      "loss @ 298 iteration: 0.17621778603906146\n",
      "loss @ 299 iteration: 0.1760337521973542\n",
      "loss @ 300 iteration: 0.17585055658568535\n",
      "loss @ 301 iteration: 0.17566819234640602\n",
      "loss @ 302 iteration: 0.175486652702132\n",
      "loss @ 303 iteration: 0.1753059309545341\n",
      "loss @ 304 iteration: 0.17512602048314976\n",
      "loss @ 305 iteration: 0.17494691474421722\n",
      "loss @ 306 iteration: 0.17476860726953058\n",
      "loss @ 307 iteration: 0.17459109166531556\n",
      "loss @ 308 iteration: 0.1744143616111259\n",
      "loss @ 309 iteration: 0.17423841085875913\n",
      "loss @ 310 iteration: 0.17406323323119297\n",
      "loss @ 311 iteration: 0.1738888226215393\n",
      "loss @ 312 iteration: 0.1737151729920182\n",
      "loss @ 313 iteration: 0.17354227837294928\n",
      "loss @ 314 iteration: 0.17337013286176162\n",
      "loss @ 315 iteration: 0.17319873062202074\n",
      "loss @ 316 iteration: 0.17302806588247327\n",
      "loss @ 317 iteration: 0.17285813293610777\n",
      "loss @ 318 iteration: 0.17268892613923262\n",
      "loss @ 319 iteration: 0.1725204399105698\n",
      "loss @ 320 iteration: 0.17235266873036456\n",
      "loss @ 321 iteration: 0.17218560713951012\n",
      "loss @ 322 iteration: 0.17201924973868846\n",
      "loss @ 323 iteration: 0.1718535911875249\n",
      "loss @ 324 iteration: 0.17168862620375816\n",
      "loss @ 325 iteration: 0.1715243495624239\n",
      "loss @ 326 iteration: 0.1713607560950529\n",
      "loss @ 327 iteration: 0.1711978406888823\n",
      "loss @ 328 iteration: 0.1710355982860809\n",
      "loss @ 329 iteration: 0.17087402388298695\n",
      "loss @ 330 iteration: 0.17071311252935945\n",
      "loss @ 331 iteration: 0.17055285932764158\n",
      "loss @ 332 iteration: 0.1703932594322368\n",
      "loss @ 333 iteration: 0.17023430804879694\n",
      "loss @ 334 iteration: 0.17007600043352225\n",
      "loss @ 335 iteration: 0.16991833189247318\n",
      "loss @ 336 iteration: 0.1697612977808933\n",
      "loss @ 337 iteration: 0.16960489350254382\n",
      "loss @ 338 iteration: 0.16944911450904893\n",
      "loss @ 339 iteration: 0.16929395629925173\n",
      "loss @ 340 iteration: 0.1691394144185811\n",
      "loss @ 341 iteration: 0.16898548445842865\n",
      "loss @ 342 iteration: 0.16883216205553583\n",
      "loss @ 343 iteration: 0.1686794428913913\n",
      "loss @ 344 iteration: 0.16852732269163775\n",
      "loss @ 345 iteration: 0.1683757972254885\n",
      "loss @ 346 iteration: 0.16822486230515335\n",
      "loss @ 347 iteration: 0.16807451378527397\n",
      "loss @ 348 iteration: 0.16792474756236794\n",
      "loss @ 349 iteration: 0.16777555957428197\n",
      "loss @ 350 iteration: 0.16762694579965384\n",
      "loss @ 351 iteration: 0.16747890225738254\n",
      "loss @ 352 iteration: 0.1673314250061075\n",
      "loss @ 353 iteration: 0.16718451014369512\n",
      "loss @ 354 iteration: 0.1670381538067342\n",
      "loss @ 355 iteration: 0.16689235217003903\n",
      "loss @ 356 iteration: 0.16674710144616012\n",
      "loss @ 357 iteration: 0.1666023978849026\n",
      "loss @ 358 iteration: 0.1664582377728525\n",
      "loss @ 359 iteration: 0.16631461743290993\n",
      "loss @ 360 iteration: 0.1661715332238295\n",
      "loss @ 361 iteration: 0.1660289815397684\n",
      "loss @ 362 iteration: 0.16588695880984078\n",
      "loss @ 363 iteration: 0.16574546149767938\n",
      "loss @ 364 iteration: 0.1656044861010037\n",
      "loss @ 365 iteration: 0.1654640291511948\n",
      "loss @ 366 iteration: 0.16532408721287664\n",
      "loss @ 367 iteration: 0.16518465688350392\n",
      "loss @ 368 iteration: 0.1650457347929556\n",
      "loss @ 369 iteration: 0.16490731760313532\n",
      "loss @ 370 iteration: 0.1647694020075774\n",
      "loss @ 371 iteration: 0.1646319847310586\n",
      "loss @ 372 iteration: 0.16449506252921636\n",
      "loss @ 373 iteration: 0.16435863218817193\n",
      "loss @ 374 iteration: 0.1642226905241599\n",
      "loss @ 375 iteration: 0.16408723438316242\n",
      "loss @ 376 iteration: 0.16395226064054977\n",
      "loss @ 377 iteration: 0.16381776620072538\n",
      "loss @ 378 iteration: 0.1636837479967766\n",
      "loss @ 379 iteration: 0.1635502029901306\n",
      "loss @ 380 iteration: 0.16341712817021492\n",
      "loss @ 381 iteration: 0.16328452055412357\n",
      "loss @ 382 iteration: 0.16315237718628742\n",
      "loss @ 383 iteration: 0.1630206951381498\n",
      "loss @ 384 iteration: 0.16288947150784672\n",
      "loss @ 385 iteration: 0.16275870341989146\n",
      "loss @ 386 iteration: 0.16262838802486412\n",
      "loss @ 387 iteration: 0.16249852249910546\n",
      "loss @ 388 iteration: 0.16236910404441493\n",
      "loss @ 389 iteration: 0.1622401298877533\n",
      "loss @ 390 iteration: 0.16211159728094968\n",
      "loss @ 391 iteration: 0.161983503500412\n",
      "loss @ 392 iteration: 0.1618558458468426\n",
      "loss @ 393 iteration: 0.16172862164495722\n",
      "loss @ 394 iteration: 0.16160182824320773\n",
      "loss @ 395 iteration: 0.16147546301350968\n",
      "loss @ 396 iteration: 0.16134952335097275\n",
      "loss @ 397 iteration: 0.16122400667363557\n",
      "loss @ 398 iteration: 0.16109891042220384\n",
      "loss @ 399 iteration: 0.1609742320597926\n",
      "loss @ 400 iteration: 0.16084996907167168\n",
      "loss @ 401 iteration: 0.16072611896501465\n",
      "loss @ 402 iteration: 0.1606026792686517\n",
      "loss @ 403 iteration: 0.1604796475328254\n",
      "loss @ 404 iteration: 0.16035702132895022\n",
      "loss @ 405 iteration: 0.16023479824937512\n",
      "loss @ 406 iteration: 0.16011297590714957\n",
      "loss @ 407 iteration: 0.15999155193579265\n",
      "loss @ 408 iteration: 0.15987052398906523\n",
      "loss @ 409 iteration: 0.1597498897407456\n",
      "loss @ 410 iteration: 0.15962964688440767\n",
      "loss @ 411 iteration: 0.15950979313320246\n",
      "loss @ 412 iteration: 0.15939032621964258\n",
      "loss @ 413 iteration: 0.1592712438953895\n",
      "loss @ 414 iteration: 0.15915254393104364\n",
      "loss @ 415 iteration: 0.15903422411593737\n",
      "loss @ 416 iteration: 0.15891628225793084\n",
      "loss @ 417 iteration: 0.15879871618321018\n",
      "loss @ 418 iteration: 0.15868152373608907\n",
      "loss @ 419 iteration: 0.15856470277881216\n",
      "loss @ 420 iteration: 0.1584482511913616\n",
      "loss @ 421 iteration: 0.158332166871266\n",
      "loss @ 422 iteration: 0.15821644773341195\n",
      "loss @ 423 iteration: 0.15810109170985773\n",
      "loss @ 424 iteration: 0.15798609674965\n",
      "loss @ 425 iteration: 0.1578714608186424\n",
      "loss @ 426 iteration: 0.15775718189931687\n",
      "loss @ 427 iteration: 0.15764325799060686\n",
      "loss @ 428 iteration: 0.1575296871077233\n",
      "loss @ 429 iteration: 0.15741646728198283\n",
      "loss @ 430 iteration: 0.1573035965606376\n",
      "loss @ 431 iteration: 0.1571910730067081\n",
      "loss @ 432 iteration: 0.15707889469881764\n",
      "loss @ 433 iteration: 0.15696705973102903\n",
      "loss @ 434 iteration: 0.1568555662126836\n",
      "loss @ 435 iteration: 0.15674441226824173\n",
      "loss @ 436 iteration: 0.1566335960371262\n",
      "loss @ 437 iteration: 0.15652311567356678\n",
      "loss @ 438 iteration: 0.15641296934644722\n",
      "loss @ 439 iteration: 0.1563031552391541\n",
      "loss @ 440 iteration: 0.15619367154942748\n",
      "loss @ 441 iteration: 0.15608451648921362\n",
      "loss @ 442 iteration: 0.15597568828451938\n",
      "loss @ 443 iteration: 0.15586718517526837\n",
      "loss @ 444 iteration: 0.15575900541515936\n",
      "loss @ 445 iteration: 0.15565114727152593\n",
      "loss @ 446 iteration: 0.15554360902519798\n",
      "loss @ 447 iteration: 0.15543638897036527\n",
      "loss @ 448 iteration: 0.15532948541444244\n",
      "loss @ 449 iteration: 0.1552228966779354\n",
      "loss @ 450 iteration: 0.15511662109431013\n",
      "loss @ 451 iteration: 0.15501065700986214\n",
      "loss @ 452 iteration: 0.15490500278358849\n",
      "loss @ 453 iteration: 0.15479965678706056\n",
      "loss @ 454 iteration: 0.15469461740429905\n",
      "loss @ 455 iteration: 0.1545898830316498\n",
      "loss @ 456 iteration: 0.15448545207766198\n",
      "loss @ 457 iteration: 0.1543813229629669\n",
      "loss @ 458 iteration: 0.15427749412015881\n",
      "loss @ 459 iteration: 0.15417396399367714\n",
      "loss @ 460 iteration: 0.15407073103968996\n",
      "loss @ 461 iteration: 0.15396779372597877\n",
      "loss @ 462 iteration: 0.1538651505318251\n",
      "loss @ 463 iteration: 0.15376279994789788\n",
      "loss @ 464 iteration: 0.15366074047614284\n",
      "loss @ 465 iteration: 0.1535589706296725\n",
      "loss @ 466 iteration: 0.15345748893265806\n",
      "loss @ 467 iteration: 0.15335629392022226\n",
      "loss @ 468 iteration: 0.15325538413833367\n",
      "loss @ 469 iteration: 0.15315475814370205\n",
      "loss @ 470 iteration: 0.15305441450367518\n",
      "loss @ 471 iteration: 0.15295435179613656\n",
      "loss @ 472 iteration: 0.15285456860940483\n",
      "loss @ 473 iteration: 0.15275506354213395\n",
      "loss @ 474 iteration: 0.15265583520321452\n",
      "loss @ 475 iteration: 0.15255688221167654\n",
      "loss @ 476 iteration: 0.15245820319659334\n",
      "loss @ 477 iteration: 0.152359796796986\n",
      "loss @ 478 iteration: 0.15226166166172986\n",
      "loss @ 479 iteration: 0.15216379644946113\n",
      "loss @ 480 iteration: 0.15206619982848532\n",
      "loss @ 481 iteration: 0.15196887047668634\n",
      "loss @ 482 iteration: 0.15187180708143677\n",
      "loss @ 483 iteration: 0.1517750083395091\n",
      "loss @ 484 iteration: 0.15167847295698814\n",
      "loss @ 485 iteration: 0.15158219964918412\n",
      "loss @ 486 iteration: 0.1514861871405471\n",
      "loss @ 487 iteration: 0.15139043416458228\n",
      "loss @ 488 iteration: 0.15129493946376601\n",
      "loss @ 489 iteration: 0.15119970178946335\n",
      "loss @ 490 iteration: 0.15110471990184568\n",
      "loss @ 491 iteration: 0.15100999256981015\n",
      "loss @ 492 iteration: 0.15091551857089952\n",
      "loss @ 493 iteration: 0.1508212966912228\n",
      "loss @ 494 iteration: 0.1507273257253773\n",
      "loss @ 495 iteration: 0.15063360447637103\n",
      "loss @ 496 iteration: 0.15054013175554626\n",
      "loss @ 497 iteration: 0.1504469063825037\n",
      "loss @ 498 iteration: 0.15035392718502805\n",
      "loss @ 499 iteration: 0.15026119299901353\n",
      "loss @ 500 iteration: 0.15016870266839102\n",
      "loss @ 501 iteration: 0.15007645504505568\n",
      "loss @ 502 iteration: 0.14998444898879515\n",
      "loss @ 503 iteration: 0.14989268336721903\n",
      "loss @ 504 iteration: 0.14980115705568867\n",
      "loss @ 505 iteration: 0.14970986893724816\n",
      "loss @ 506 iteration: 0.14961881790255568\n",
      "loss @ 507 iteration: 0.14952800284981582\n",
      "loss @ 508 iteration: 0.1494374226847125\n",
      "loss @ 509 iteration: 0.14934707632034303\n",
      "loss @ 510 iteration: 0.14925696267715208\n",
      "loss @ 511 iteration: 0.14916708068286735\n",
      "loss @ 512 iteration: 0.14907742927243497\n",
      "loss @ 513 iteration: 0.14898800738795648\n",
      "loss @ 514 iteration: 0.1488988139786258\n",
      "loss @ 515 iteration: 0.1488098480006673\n",
      "loss @ 516 iteration: 0.1487211084172743\n",
      "loss @ 517 iteration: 0.14863259419854827\n",
      "loss @ 518 iteration: 0.14854430432143897\n",
      "loss @ 519 iteration: 0.1484562377696847\n",
      "loss @ 520 iteration: 0.14836839353375358\n",
      "loss @ 521 iteration: 0.14828077061078537\n",
      "loss @ 522 iteration: 0.14819336800453392\n",
      "loss @ 523 iteration: 0.14810618472531006\n",
      "loss @ 524 iteration: 0.14801921978992533\n",
      "loss @ 525 iteration: 0.14793247222163614\n",
      "loss @ 526 iteration: 0.14784594105008858\n",
      "loss @ 527 iteration: 0.1477596253112639\n",
      "loss @ 528 iteration: 0.14767352404742431\n",
      "loss @ 529 iteration: 0.14758763630705965\n",
      "loss @ 530 iteration: 0.1475019611448343\n",
      "loss @ 531 iteration: 0.147416497621535\n",
      "loss @ 532 iteration: 0.1473312448040189\n",
      "loss @ 533 iteration: 0.1472462017651623\n",
      "loss @ 534 iteration: 0.14716136758380996\n",
      "loss @ 535 iteration: 0.14707674134472482\n",
      "loss @ 536 iteration: 0.1469923221385382\n",
      "loss @ 537 iteration: 0.14690810906170076\n",
      "loss @ 538 iteration: 0.14682410121643377\n",
      "loss @ 539 iteration: 0.1467402977106808\n",
      "loss @ 540 iteration: 0.14665669765806016\n",
      "loss @ 541 iteration: 0.1465733001778177\n",
      "loss @ 542 iteration: 0.14649010439477983\n",
      "loss @ 543 iteration: 0.14640710943930768\n",
      "loss @ 544 iteration: 0.14632431444725086\n",
      "loss @ 545 iteration: 0.14624171855990262\n",
      "loss @ 546 iteration: 0.14615932092395453\n",
      "loss @ 547 iteration: 0.14607712069145234\n",
      "loss @ 548 iteration: 0.14599511701975212\n",
      "loss @ 549 iteration: 0.1459133090714764\n",
      "loss @ 550 iteration: 0.14583169601447132\n",
      "loss @ 551 iteration: 0.14575027702176407\n",
      "loss @ 552 iteration: 0.1456690512715204\n",
      "loss @ 553 iteration: 0.14558801794700313\n",
      "loss @ 554 iteration: 0.1455071762365305\n",
      "loss @ 555 iteration: 0.14542652533343536\n",
      "loss @ 556 iteration: 0.1453460644360246\n",
      "loss @ 557 iteration: 0.14526579274753895\n",
      "loss @ 558 iteration: 0.1451857094761132\n",
      "loss @ 559 iteration: 0.145105813834737\n",
      "loss @ 560 iteration: 0.1450261050412156\n",
      "loss @ 561 iteration: 0.14494658231813148\n",
      "loss @ 562 iteration: 0.14486724489280617\n",
      "loss @ 563 iteration: 0.14478809199726209\n",
      "loss @ 564 iteration: 0.14470912286818557\n",
      "loss @ 565 iteration: 0.14463033674688927\n",
      "loss @ 566 iteration: 0.1445517328792758\n",
      "loss @ 567 iteration: 0.14447331051580112\n",
      "loss @ 568 iteration: 0.1443950689114386\n",
      "loss @ 569 iteration: 0.14431700732564343\n",
      "loss @ 570 iteration: 0.14423912502231703\n",
      "loss @ 571 iteration: 0.14416142126977236\n",
      "loss @ 572 iteration: 0.14408389534069913\n",
      "loss @ 573 iteration: 0.1440065465121294\n",
      "loss @ 574 iteration: 0.14392937406540376\n",
      "loss @ 575 iteration: 0.1438523772861376\n",
      "loss @ 576 iteration: 0.14377555546418772\n",
      "loss @ 577 iteration: 0.14369890789361942\n",
      "loss @ 578 iteration: 0.1436224338726737\n",
      "loss @ 579 iteration: 0.14354613270373492\n",
      "loss @ 580 iteration: 0.1434700036932987\n",
      "loss @ 581 iteration: 0.1433940461519402\n",
      "loss @ 582 iteration: 0.14331825939428253\n",
      "loss @ 583 iteration: 0.14324264273896573\n",
      "loss @ 584 iteration: 0.1431671955086157\n",
      "loss @ 585 iteration: 0.14309191702981378\n",
      "loss @ 586 iteration: 0.14301680663306643\n",
      "loss @ 587 iteration: 0.14294186365277503\n",
      "loss @ 588 iteration: 0.14286708742720639\n",
      "loss @ 589 iteration: 0.14279247729846314\n",
      "loss @ 590 iteration: 0.14271803261245458\n",
      "loss @ 591 iteration: 0.1426437527188679\n",
      "loss @ 592 iteration: 0.14256963697113925\n",
      "loss @ 593 iteration: 0.1424956847264257\n",
      "loss @ 594 iteration: 0.14242189534557698\n",
      "loss @ 595 iteration: 0.1423482681931075\n",
      "loss @ 596 iteration: 0.14227480263716902\n",
      "loss @ 597 iteration: 0.14220149804952317\n",
      "loss @ 598 iteration: 0.1421283538055144\n",
      "loss @ 599 iteration: 0.1420553692840431\n",
      "loss @ 600 iteration: 0.14198254386753922\n",
      "loss @ 601 iteration: 0.14190987694193552\n",
      "loss @ 602 iteration: 0.14183736789664192\n",
      "loss @ 603 iteration: 0.1417650161245194\n",
      "loss @ 604 iteration: 0.14169282102185435\n",
      "loss @ 605 iteration: 0.14162078198833333\n",
      "loss @ 606 iteration: 0.14154889842701776\n",
      "loss @ 607 iteration: 0.1414771697443192\n",
      "loss @ 608 iteration: 0.1414055953499744\n",
      "loss @ 609 iteration: 0.14133417465702097\n",
      "loss @ 610 iteration: 0.14126290708177303\n",
      "loss @ 611 iteration: 0.14119179204379734\n",
      "loss @ 612 iteration: 0.14112082896588923\n",
      "loss @ 613 iteration: 0.1410500172740493\n",
      "loss @ 614 iteration: 0.1409793563974597\n",
      "loss @ 615 iteration: 0.1409088457684612\n",
      "loss @ 616 iteration: 0.14083848482253003\n",
      "loss @ 617 iteration: 0.14076827299825523\n",
      "loss @ 618 iteration: 0.14069820973731617\n",
      "loss @ 619 iteration: 0.14062829448445982\n",
      "loss @ 620 iteration: 0.14055852668747904\n",
      "loss @ 621 iteration: 0.14048890579719017\n",
      "loss @ 622 iteration: 0.14041943126741174\n",
      "loss @ 623 iteration: 0.14035010255494232\n",
      "loss @ 624 iteration: 0.14028091911953974\n",
      "loss @ 625 iteration: 0.14021188042389934\n",
      "loss @ 626 iteration: 0.14014298593363325\n",
      "loss @ 627 iteration: 0.14007423511724956\n",
      "loss @ 628 iteration: 0.14000562744613154\n",
      "loss @ 629 iteration: 0.13993716239451726\n",
      "loss @ 630 iteration: 0.13986883943947925\n",
      "loss @ 631 iteration: 0.1398006580609045\n",
      "loss @ 632 iteration: 0.13973261774147444\n",
      "loss @ 633 iteration: 0.139664717966645\n",
      "loss @ 634 iteration: 0.1395969582246275\n",
      "loss @ 635 iteration: 0.1395293380063687\n",
      "loss @ 636 iteration: 0.13946185680553191\n",
      "loss @ 637 iteration: 0.13939451411847778\n",
      "loss @ 638 iteration: 0.13932730944424526\n",
      "loss @ 639 iteration: 0.1392602422845331\n",
      "loss @ 640 iteration: 0.13919331214368103\n",
      "loss @ 641 iteration: 0.13912651852865146\n",
      "loss @ 642 iteration: 0.13905986094901124\n",
      "loss @ 643 iteration: 0.13899333891691323\n",
      "loss @ 644 iteration: 0.1389269519470788\n",
      "loss @ 645 iteration: 0.13886069955677974\n",
      "loss @ 646 iteration: 0.13879458126582048\n",
      "loss @ 647 iteration: 0.138728596596521\n",
      "loss @ 648 iteration: 0.13866274507369905\n",
      "loss @ 649 iteration: 0.13859702622465314\n",
      "loss @ 650 iteration: 0.13853143957914546\n",
      "loss @ 651 iteration: 0.138465984669385\n",
      "loss @ 652 iteration: 0.1384006610300106\n",
      "loss @ 653 iteration: 0.13833546819807452\n",
      "loss @ 654 iteration: 0.13827040571302582\n",
      "loss @ 655 iteration: 0.13820547311669398\n",
      "loss @ 656 iteration: 0.13814066995327287\n",
      "loss @ 657 iteration: 0.13807599576930446\n",
      "loss @ 658 iteration: 0.1380114501136629\n",
      "loss @ 659 iteration: 0.13794703253753876\n",
      "loss @ 660 iteration: 0.13788274259442346\n",
      "loss @ 661 iteration: 0.13781857984009344\n",
      "loss @ 662 iteration: 0.13775454383259494\n",
      "loss @ 663 iteration: 0.13769063413222868\n",
      "loss @ 664 iteration: 0.13762685030153465\n",
      "loss @ 665 iteration: 0.13756319190527716\n",
      "loss @ 666 iteration: 0.1374996585104297\n",
      "loss @ 667 iteration: 0.1374362496861604\n",
      "loss @ 668 iteration: 0.13737296500381718\n",
      "loss @ 669 iteration: 0.1373098040369134\n",
      "loss @ 670 iteration: 0.13724676636111313\n",
      "loss @ 671 iteration: 0.13718385155421717\n",
      "loss @ 672 iteration: 0.13712105919614853\n",
      "loss @ 673 iteration: 0.13705838886893867\n",
      "loss @ 674 iteration: 0.13699584015671326\n",
      "loss @ 675 iteration: 0.13693341264567852\n",
      "loss @ 676 iteration: 0.1368711059241073\n",
      "loss @ 677 iteration: 0.13680891958232574\n",
      "loss @ 678 iteration: 0.1367468532126994\n",
      "loss @ 679 iteration: 0.1366849064096201\n",
      "loss @ 680 iteration: 0.13662307876949265\n",
      "loss @ 681 iteration: 0.1365613698907214\n",
      "loss @ 682 iteration: 0.13649977937369745\n",
      "loss @ 683 iteration: 0.1364383068207855\n",
      "loss @ 684 iteration: 0.13637695183631104\n",
      "loss @ 685 iteration: 0.13631571402654755\n",
      "loss @ 686 iteration: 0.13625459299970386\n",
      "loss @ 687 iteration: 0.1361935883659116\n",
      "loss @ 688 iteration: 0.13613269973721268\n",
      "loss @ 689 iteration: 0.13607192672754692\n",
      "loss @ 690 iteration: 0.13601126895273996\n",
      "loss @ 691 iteration: 0.13595072603049074\n",
      "loss @ 692 iteration: 0.13589029758035984\n",
      "loss @ 693 iteration: 0.13582998322375725\n",
      "loss @ 694 iteration: 0.13576978258393044\n",
      "loss @ 695 iteration: 0.13570969528595286\n",
      "loss @ 696 iteration: 0.13564972095671196\n",
      "loss @ 697 iteration: 0.1355898592248977\n",
      "loss @ 698 iteration: 0.135530109720991\n",
      "loss @ 699 iteration: 0.13547047207725246\n",
      "loss @ 700 iteration: 0.13541094592771089\n",
      "loss @ 701 iteration: 0.13535153090815213\n",
      "loss @ 702 iteration: 0.1352922266561078\n",
      "loss @ 703 iteration: 0.13523303281084445\n",
      "loss @ 704 iteration: 0.13517394901335228\n",
      "loss @ 705 iteration: 0.13511497490633465\n",
      "loss @ 706 iteration: 0.1350561101341968\n",
      "loss @ 707 iteration: 0.13499735434303553\n",
      "loss @ 708 iteration: 0.13493870718062834\n",
      "loss @ 709 iteration: 0.13488016829642302\n",
      "loss @ 710 iteration: 0.13482173734152686\n",
      "loss @ 711 iteration: 0.1347634139686968\n",
      "loss @ 712 iteration: 0.13470519783232854\n",
      "loss @ 713 iteration: 0.1346470885884468\n",
      "loss @ 714 iteration: 0.13458908589469465\n",
      "loss @ 715 iteration: 0.13453118941032402\n",
      "loss @ 716 iteration: 0.13447339879618517\n",
      "loss @ 717 iteration: 0.13441571371471717\n",
      "loss @ 718 iteration: 0.13435813382993775\n",
      "loss @ 719 iteration: 0.13430065880743375\n",
      "loss @ 720 iteration: 0.13424328831435126\n",
      "loss @ 721 iteration: 0.13418602201938612\n",
      "loss @ 722 iteration: 0.13412885959277437\n",
      "loss @ 723 iteration: 0.1340718007062826\n",
      "loss @ 724 iteration: 0.13401484503319883\n",
      "loss @ 725 iteration: 0.13395799224832286\n",
      "loss @ 726 iteration: 0.1339012420279573\n",
      "loss @ 727 iteration: 0.1338445940498982\n",
      "loss @ 728 iteration: 0.13378804799342597\n",
      "loss @ 729 iteration: 0.1337316035392963\n",
      "loss @ 730 iteration: 0.13367526036973132\n",
      "loss @ 731 iteration: 0.13361901816841049\n",
      "loss @ 732 iteration: 0.13356287662046176\n",
      "loss @ 733 iteration: 0.133506835412453\n",
      "loss @ 734 iteration: 0.13345089423238313\n",
      "loss @ 735 iteration: 0.13339505276967328\n",
      "loss @ 736 iteration: 0.1333393107151586\n",
      "loss @ 737 iteration: 0.1332836677610795\n",
      "loss @ 738 iteration: 0.1332281236010731\n",
      "loss @ 739 iteration: 0.13317267793016516\n",
      "loss @ 740 iteration: 0.13311733044476132\n",
      "loss @ 741 iteration: 0.13306208084263915\n",
      "loss @ 742 iteration: 0.13300692882293988\n",
      "loss @ 743 iteration: 0.13295187408616016\n",
      "loss @ 744 iteration: 0.13289691633414397\n",
      "loss @ 745 iteration: 0.13284205527007464\n",
      "loss @ 746 iteration: 0.13278729059846697\n",
      "loss @ 747 iteration: 0.13273262202515906\n",
      "loss @ 748 iteration: 0.13267804925730467\n",
      "loss @ 749 iteration: 0.1326235720033655\n",
      "loss @ 750 iteration: 0.13256918997310307\n",
      "loss @ 751 iteration: 0.13251490287757145\n",
      "loss @ 752 iteration: 0.13246071042910945\n",
      "loss @ 753 iteration: 0.13240661234133297\n",
      "loss @ 754 iteration: 0.13235260832912762\n",
      "loss @ 755 iteration: 0.13229869810864126\n",
      "loss @ 756 iteration: 0.1322448813972765\n",
      "loss @ 757 iteration: 0.13219115791368333\n",
      "loss @ 758 iteration: 0.13213752737775197\n",
      "loss @ 759 iteration: 0.13208398951060557\n",
      "loss @ 760 iteration: 0.1320305440345928\n",
      "loss @ 761 iteration: 0.1319771906732811\n",
      "loss @ 762 iteration: 0.13192392915144918\n",
      "loss @ 763 iteration: 0.13187075919508032\n",
      "loss @ 764 iteration: 0.13181768053135515\n",
      "loss @ 765 iteration: 0.1317646928886448\n",
      "loss @ 766 iteration: 0.13171179599650412\n",
      "loss @ 767 iteration: 0.13165898958566452\n",
      "loss @ 768 iteration: 0.1316062733880275\n",
      "loss @ 769 iteration: 0.13155364713665782\n",
      "loss @ 770 iteration: 0.13150111056577668\n",
      "loss @ 771 iteration: 0.13144866341075526\n",
      "loss @ 772 iteration: 0.13139630540810798\n",
      "loss @ 773 iteration: 0.13134403629548594\n",
      "loss @ 774 iteration: 0.13129185581167063\n",
      "loss @ 775 iteration: 0.13123976369656715\n",
      "loss @ 776 iteration: 0.13118775969119797\n",
      "loss @ 777 iteration: 0.13113584353769664\n",
      "loss @ 778 iteration: 0.1310840149793013\n",
      "loss @ 779 iteration: 0.1310322737603485\n",
      "loss @ 780 iteration: 0.13098061962626692\n",
      "loss @ 781 iteration: 0.1309290523235712\n",
      "loss @ 782 iteration: 0.13087757159985572\n",
      "loss @ 783 iteration: 0.13082617720378875\n",
      "loss @ 784 iteration: 0.13077486888510598\n",
      "loss @ 785 iteration: 0.13072364639460485\n",
      "loss @ 786 iteration: 0.13067250948413842\n",
      "loss @ 787 iteration: 0.13062145790660942\n",
      "loss @ 788 iteration: 0.13057049141596447\n",
      "loss @ 789 iteration: 0.13051960976718807\n",
      "loss @ 790 iteration: 0.13046881271629687\n",
      "loss @ 791 iteration: 0.130418100020334\n",
      "loss @ 792 iteration: 0.1303674714373631\n",
      "loss @ 793 iteration: 0.13031692672646286\n",
      "loss @ 794 iteration: 0.13026646564772115\n",
      "loss @ 795 iteration: 0.13021608796222964\n",
      "loss @ 796 iteration: 0.1301657934320781\n",
      "loss @ 797 iteration: 0.13011558182034877\n",
      "loss @ 798 iteration: 0.13006545289111107\n",
      "loss @ 799 iteration: 0.13001540640941592\n",
      "loss @ 800 iteration: 0.12996544214129058\n",
      "loss @ 801 iteration: 0.129915559853733\n",
      "loss @ 802 iteration: 0.12986575931470654\n",
      "loss @ 803 iteration: 0.12981604029313482\n",
      "loss @ 804 iteration: 0.12976640255889624\n",
      "loss @ 805 iteration: 0.12971684588281884\n",
      "loss @ 806 iteration: 0.12966737003667508\n",
      "loss @ 807 iteration: 0.12961797479317666\n",
      "loss @ 808 iteration: 0.1295686599259694\n",
      "loss @ 809 iteration: 0.12951942520962803\n",
      "loss @ 810 iteration: 0.12947027041965148\n",
      "loss @ 811 iteration: 0.1294211953324573\n",
      "loss @ 812 iteration: 0.12937219972537714\n",
      "loss @ 813 iteration: 0.12932328337665155\n",
      "loss @ 814 iteration: 0.1292744460654251\n",
      "loss @ 815 iteration: 0.1292256875717415\n",
      "loss @ 816 iteration: 0.12917700767653875\n",
      "loss @ 817 iteration: 0.1291284061616442\n",
      "loss @ 818 iteration: 0.12907988280976995\n",
      "loss @ 819 iteration: 0.12903143740450784\n",
      "loss @ 820 iteration: 0.12898306973032495\n",
      "loss @ 821 iteration: 0.12893477957255878\n",
      "loss @ 822 iteration: 0.12888656671741253\n",
      "loss @ 823 iteration: 0.12883843095195058\n",
      "loss @ 824 iteration: 0.12879037206409374\n",
      "loss @ 825 iteration: 0.12874238984261485\n",
      "loss @ 826 iteration: 0.12869448407713416\n",
      "loss @ 827 iteration: 0.12864665455811455\n",
      "loss @ 828 iteration: 0.12859890107685756\n",
      "loss @ 829 iteration: 0.12855122342549852\n",
      "loss @ 830 iteration: 0.12850362139700222\n",
      "loss @ 831 iteration: 0.12845609478515857\n",
      "loss @ 832 iteration: 0.12840864338457822\n",
      "loss @ 833 iteration: 0.12836126699068817\n",
      "loss @ 834 iteration: 0.1283139653997275\n",
      "loss @ 835 iteration: 0.12826673840874306\n",
      "loss @ 836 iteration: 0.1282195858155852\n",
      "loss @ 837 iteration: 0.1281725074189036\n",
      "loss @ 838 iteration: 0.12812550301814293\n",
      "loss @ 839 iteration: 0.12807857241353895\n",
      "loss @ 840 iteration: 0.12803171540611402\n",
      "loss @ 841 iteration: 0.1279849317976732\n",
      "loss @ 842 iteration: 0.1279382213908002\n",
      "loss @ 843 iteration: 0.12789158398885303\n",
      "loss @ 844 iteration: 0.12784501939596032\n",
      "loss @ 845 iteration: 0.12779852741701703\n",
      "loss @ 846 iteration: 0.12775210785768062\n",
      "loss @ 847 iteration: 0.12770576052436697\n",
      "loss @ 848 iteration: 0.12765948522424664\n",
      "loss @ 849 iteration: 0.1276132817652407\n",
      "loss @ 850 iteration: 0.12756714995601695\n",
      "loss @ 851 iteration: 0.12752108960598618\n",
      "loss @ 852 iteration: 0.1274751005252982\n",
      "loss @ 853 iteration: 0.12742918252483804\n",
      "loss @ 854 iteration: 0.12738333541622215\n",
      "loss @ 855 iteration: 0.1273375590117948\n",
      "loss @ 856 iteration: 0.12729185312462407\n",
      "loss @ 857 iteration: 0.1272462175684984\n",
      "loss @ 858 iteration: 0.1272006521579227\n",
      "loss @ 859 iteration: 0.12715515670811472\n",
      "loss @ 860 iteration: 0.12710973103500156\n",
      "loss @ 861 iteration: 0.1270643749552159\n",
      "loss @ 862 iteration: 0.1270190882860924\n",
      "loss @ 863 iteration: 0.12697387084566414\n",
      "loss @ 864 iteration: 0.12692872245265918\n",
      "loss @ 865 iteration: 0.12688364292649693\n",
      "loss @ 866 iteration: 0.12683863208728452\n",
      "loss @ 867 iteration: 0.12679368975581357\n",
      "loss @ 868 iteration: 0.12674881575355662\n",
      "loss @ 869 iteration: 0.12670400990266356\n",
      "loss @ 870 iteration: 0.12665927202595845\n",
      "loss @ 871 iteration: 0.12661460194693594\n",
      "loss @ 872 iteration: 0.12656999948975786\n",
      "loss @ 873 iteration: 0.1265254644792501\n",
      "loss @ 874 iteration: 0.126480996740899\n",
      "loss @ 875 iteration: 0.12643659610084823\n",
      "loss @ 876 iteration: 0.12639226238589546\n",
      "loss @ 877 iteration: 0.12634799542348893\n",
      "loss @ 878 iteration: 0.12630379504172448\n",
      "loss @ 879 iteration: 0.126259661069342\n",
      "loss @ 880 iteration: 0.1262155933357225\n",
      "loss @ 881 iteration: 0.12617159167088474\n",
      "loss @ 882 iteration: 0.12612765590548214\n",
      "loss @ 883 iteration: 0.12608378587079952\n",
      "loss @ 884 iteration: 0.12603998139875022\n",
      "loss @ 885 iteration: 0.12599624232187265\n",
      "loss @ 886 iteration: 0.1259525684733274\n",
      "loss @ 887 iteration: 0.1259089596868941\n",
      "loss @ 888 iteration: 0.12586541579696853\n",
      "loss @ 889 iteration: 0.12582193663855926\n",
      "loss @ 890 iteration: 0.1257785220472849\n",
      "loss @ 891 iteration: 0.12573517185937097\n",
      "loss @ 892 iteration: 0.125691885911647\n",
      "loss @ 893 iteration: 0.12564866404154354\n",
      "loss @ 894 iteration: 0.1256055060870891\n",
      "loss @ 895 iteration: 0.12556241188690734\n",
      "loss @ 896 iteration: 0.12551938128021425\n",
      "loss @ 897 iteration: 0.12547641410681507\n",
      "loss @ 898 iteration: 0.12543351020710147\n",
      "loss @ 899 iteration: 0.12539066942204874\n",
      "loss @ 900 iteration: 0.1253478915932129\n",
      "loss @ 901 iteration: 0.125305176562728\n",
      "loss @ 902 iteration: 0.1252625241733031\n",
      "loss @ 903 iteration: 0.12521993426821965\n",
      "loss @ 904 iteration: 0.12517740669132862\n",
      "loss @ 905 iteration: 0.12513494128704794\n",
      "loss @ 906 iteration: 0.1250925379003594\n",
      "loss @ 907 iteration: 0.12505019637680634\n",
      "loss @ 908 iteration: 0.12500791656249055\n",
      "loss @ 909 iteration: 0.12496569830406999\n",
      "loss @ 910 iteration: 0.12492354144875573\n",
      "loss @ 911 iteration: 0.12488144584430962\n",
      "loss @ 912 iteration: 0.12483941133904129\n",
      "loss @ 913 iteration: 0.12479743778180599\n",
      "loss @ 914 iteration: 0.12475552502200152\n",
      "loss @ 915 iteration: 0.12471367290956592\n",
      "loss @ 916 iteration: 0.12467188129497478\n",
      "loss @ 917 iteration: 0.12463015002923875\n",
      "loss @ 918 iteration: 0.12458847896390088\n",
      "loss @ 919 iteration: 0.12454686795103426\n",
      "loss @ 920 iteration: 0.1245053168432393\n",
      "loss @ 921 iteration: 0.12446382549364134\n",
      "loss @ 922 iteration: 0.12442239375588819\n",
      "loss @ 923 iteration: 0.1243810214841476\n",
      "loss @ 924 iteration: 0.12433970853310479\n",
      "loss @ 925 iteration: 0.12429845475796007\n",
      "loss @ 926 iteration: 0.12425726001442632\n",
      "loss @ 927 iteration: 0.12421612415872665\n",
      "loss @ 928 iteration: 0.12417504704759204\n",
      "loss @ 929 iteration: 0.12413402853825878\n",
      "loss @ 930 iteration: 0.12409306848846625\n",
      "loss @ 931 iteration: 0.12405216675645453\n",
      "loss @ 932 iteration: 0.12401132320096198\n",
      "loss @ 933 iteration: 0.12397053768122314\n",
      "loss @ 934 iteration: 0.123929810056966\n",
      "loss @ 935 iteration: 0.12388914018841005\n",
      "loss @ 936 iteration: 0.12384852793626397\n",
      "loss @ 937 iteration: 0.12380797316172305\n",
      "loss @ 938 iteration: 0.12376747572646732\n",
      "loss @ 939 iteration: 0.12372703549265902\n",
      "loss @ 940 iteration: 0.12368665232294049\n",
      "loss @ 941 iteration: 0.1236463260804318\n",
      "loss @ 942 iteration: 0.12360605662872882\n",
      "loss @ 943 iteration: 0.12356584383190071\n",
      "loss @ 944 iteration: 0.12352568755448796\n",
      "loss @ 945 iteration: 0.1234855876615\n",
      "loss @ 946 iteration: 0.1234455440184133\n",
      "loss @ 947 iteration: 0.12340555649116902\n",
      "loss @ 948 iteration: 0.12336562494617091\n",
      "loss @ 949 iteration: 0.12332574925028329\n",
      "loss @ 950 iteration: 0.12328592927082871\n",
      "loss @ 951 iteration: 0.12324616487558619\n",
      "loss @ 952 iteration: 0.12320645593278884\n",
      "loss @ 953 iteration: 0.12316680231112184\n",
      "loss @ 954 iteration: 0.12312720387972052\n",
      "loss @ 955 iteration: 0.12308766050816813\n",
      "loss @ 956 iteration: 0.12304817206649389\n",
      "loss @ 957 iteration: 0.123008738425171\n",
      "loss @ 958 iteration: 0.12296935945511442\n",
      "loss @ 959 iteration: 0.12293003502767924\n",
      "loss @ 960 iteration: 0.12289076501465826\n",
      "loss @ 961 iteration: 0.1228515492882803\n",
      "loss @ 962 iteration: 0.12281238772120809\n",
      "loss @ 963 iteration: 0.12277328018653634\n",
      "loss @ 964 iteration: 0.12273422655778983\n",
      "loss @ 965 iteration: 0.12269522670892144\n",
      "loss @ 966 iteration: 0.12265628051431023\n",
      "loss @ 967 iteration: 0.12261738784875942\n",
      "loss @ 968 iteration: 0.12257854858749473\n",
      "loss @ 969 iteration: 0.12253976260616217\n",
      "loss @ 970 iteration: 0.12250102978082646\n",
      "loss @ 971 iteration: 0.12246234998796889\n",
      "loss @ 972 iteration: 0.12242372310448567\n",
      "loss @ 973 iteration: 0.1223851490076859\n",
      "loss @ 974 iteration: 0.12234662757528988\n",
      "loss @ 975 iteration: 0.12230815868542717\n",
      "loss @ 976 iteration: 0.12226974221663478\n",
      "loss @ 977 iteration: 0.12223137804785544\n",
      "loss @ 978 iteration: 0.12219306605843566\n",
      "loss @ 979 iteration: 0.12215480612812407\n",
      "loss @ 980 iteration: 0.12211659813706963\n",
      "loss @ 981 iteration: 0.12207844196581966\n",
      "loss @ 982 iteration: 0.12204033749531833\n",
      "loss @ 983 iteration: 0.12200228460690482\n",
      "loss @ 984 iteration: 0.1219642831823115\n",
      "loss @ 985 iteration: 0.12192633310366228\n",
      "loss @ 986 iteration: 0.12188843425347082\n",
      "loss @ 987 iteration: 0.12185058651463894\n",
      "loss @ 988 iteration: 0.12181278977045479\n",
      "loss @ 989 iteration: 0.12177504390459111\n",
      "loss @ 990 iteration: 0.12173734880110375\n",
      "loss @ 991 iteration: 0.12169970434442984\n",
      "loss @ 992 iteration: 0.12166211041938613\n",
      "loss @ 993 iteration: 0.12162456691116724\n",
      "loss @ 994 iteration: 0.12158707370534441\n",
      "loss @ 995 iteration: 0.12154963068786331\n",
      "loss @ 996 iteration: 0.12151223774504273\n",
      "loss @ 997 iteration: 0.12147489476357294\n",
      "loss @ 998 iteration: 0.12143760163051404\n",
      "loss @ 999 iteration: 0.12140035823329427\n",
      "loss @ 1000 iteration: 0.12136316445970856\n",
      "final parameters: w = [[-0.52061891 -1.42155256 -0.70508007 -2.33344502  1.47077546 -0.72186957\n",
      "  -3.05350461 -4.67796602  1.53735418  4.30814707 -2.2662968   0.1798238\n",
      "  -1.71439297 -1.75196078  0.43687093  1.5146345   1.01155409  0.32190121\n",
      "   0.9239979   1.49172704 -2.03051206 -2.63811585 -1.95618587 -3.12680423\n",
      "  -0.17016846 -1.60354768 -2.35558862 -5.01114019 -0.78654638  1.078621  ]], b = 6.27928480560748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7e1cbc540050>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "w = np.random.normal(0, 1e-4, (1, features_train.shape[1])) \n",
    "b = np.random.normal(0, 1e-4)\n",
    "print(f\"Initial parameters: w = {w}, b = {b}\")\n",
    "\n",
    "# Set training hyperparameters\n",
    "num_iters = 1000\n",
    "learning_rate = 0.8\n",
    "losses = []\n",
    "\n",
    "# Gradient descent loop\n",
    "for i in range(num_iters):\n",
    "    preds_train = forward(features_train, w, b)\n",
    "    loss = bce_loss(preds_train, labels_train)\n",
    "    print(f\"loss @ {i+1} iteration: {loss}\")\n",
    "    losses.append(loss)\n",
    "    dw, db = grad(features_train, preds_train, labels_train)\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "print(f\"final parameters: w = {w}, b = {b}\")\n",
    "\n",
    "# Visualize training losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model accuracy: 0.9648506151142355\n",
      "Samples of trained model's predictions:\n",
      " [[1.02604437e-01]\n",
      " [8.70495140e-06]\n",
      " [9.97068536e-01]]\n",
      "Samples of trained model's classifications:\n",
      " [[False]\n",
      " [False]\n",
      " [ True]]\n",
      "Samples of corresponding labels:\n",
      " [[0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "predclas_train = preds_train > 0.5\n",
    "acc_train = np.sum(predclas_train==labels_train) / labels_train.shape[0]\n",
    "print(f\"Trained model accuracy: {acc_train}\")\n",
    "print(f\"Samples of trained model's predictions:\\n {preds_train[-3:]}\")\n",
    "print(f\"Samples of trained model's classifications:\\n {predclas_train[-3:]}\")\n",
    "print(f\"Samples of corresponding labels:\\n {labels_train[-3:]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binary-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
