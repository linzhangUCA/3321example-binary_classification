{"cells":[{"cell_type":"markdown","metadata":{"id":"WXjKp5Q-lksO"},"source":["# Binary Classification\n","In this example, we will walk through two-category classification problem.\n","\n","The following contents will be covered:\n","1. \n"]},{"cell_type":"markdown","metadata":{"id":"quxxsYbvlksR"},"source":["## Load Raw Data\n","[scikit-learn](https://scikit-learn.org/stable/index.html) is a handy machine learning library for Python. We can creat an artificial dataset for the binary classification task using its [datasets](https://scikit-learn.org/stable/api/sklearn.datasets.html) module."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1694659674888,"user":{"displayName":"Lin Zhang","userId":"02863611533796321015"},"user_tz":300},"id":"2NFcnJf6lksS","outputId":"6680e9e7-7388-45e7-bb78-068a4ea42b8f"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe kernel failed to start due to the missing module 'pexpect'. Consider installing this module.\n","\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."]}],"source":["from sklearn.datasets import make_moons\n","import numpy as np\n","X, y = make_moons(n_samples=1024, noise=0.2, random_state=3321)\n","print(X.shape, y.shape)\n","print(X[:10])\n","print(y[:10])"]},{"cell_type":"markdown","metadata":{"id":"ID3xPmPDlksU"},"source":["### Visualize the Data Feature-Wise\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"elapsed":987,"status":"ok","timestamp":1694659681625,"user":{"displayName":"Lin Zhang","userId":"02863611533796321015"},"user_tz":300},"id":"7rP5n86rlksU","outputId":"3e0d73b2-d956-4d1d-b2fd-bb8a195f2ddf"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Regulate the Data\n","1. Rescale\n","2. Reshape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Rescale \n","# age_train_rescale = age_train / 10\n","# mileage_train_rescale = mileage_train / 1e4\n","# price_train_rescale = price_train / 1e4\n","# age_test_rescale = age_test / 10\n","# mileage_test_rescale = mileage_test / 1e4\n","# price_test_rescale = price_test / 1e4\n","\n","# Rescale by max\n","X_train = X / X.max(axis=0)\n","\n","# Reshape labels\n","y_train = y.reshape(-1, 1)\n","print(X_train.shape, y_train.shape)  # for debug\n","\n","# Visualize\n","plt.scatter(X_train[:, 0], X_train[:, 1], s=5*np.ones(y_train.size), c=y_train, cmap='coolwarm')"]},{"cell_type":"markdown","metadata":{"id":"i1oW67_4lksU"},"source":["## Create a Linear Model\n","A linear model: $\\hat{y}=w_1x_1 + w_2x_2 + \\dots + w_Nx_N + b$ can predict an entity's property based on its $N$ features. Represent the such a linear model in matrix format leads to:\n"," \n","$\\hat{\\mathbf{y}} = \\mathbf{X} \\cdot \\mathbf{w}^T + \\mathbf{b}$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"elapsed":813,"status":"ok","timestamp":1694659685939,"user":{"displayName":"Lin Zhang","userId":"02863611533796321015"},"user_tz":300},"id":"9O5HK-sYlksV","outputId":"8a5da8e8-b1b5-438e-ab6b-d9b95b2dc75f"},"outputs":[],"source":["# Define model function\n","def linear(feature, weight, bias):\n","    \"\"\" Model function\n","    Args:\n","        input: feature matrix (independent variables), 2d-array with shape (samples #, features #)\n","        weight: row vector of weights, 2d-array with shape (1, # features)\n","        bias: scalar\n","    Returns:\n","        output: column vector of predictions (dependent variables), 2d-array with shape (# samples, 1)\n","    \"\"\"\n","    pred = feature @ weight.T + bias\n","    return pred\n","\n","# Sanity check\n","w_dummy = np.array([[1.2, -2.3]]) \n","b_dummy = 3.4\n","y_pred_dummy = linear(X_train, w_dummy, b_dummy)\n","\n","\n","# Decision boundary\n","db_x = np.linspace(-3., 3.)\n","db_y = (0.5 - b_dummy - w_dummy[0, 0] * db_x) / w_dummy[0, 1]\n","plt.scatter(X_train[:, 0], X_train[:, 1], s=5*np.ones(y_train.size), c=y_train, cmap='coolwarm')\n","plt.plot(db_x, db_y, 'k')\n","# Create a meshgrid of points over the feature space\n","xx, yy = np.meshgrid(np.linspace(-3., 3., 300),\n","                     np.linspace(-3., 3., 300))\n","zz = w_dummy[0, 0] * xx + w_dummy[0, 1] * yy + b_dummy - 0.5\n","plt.contourf(xx, yy, zz, levels=[-np.inf, 0, np.inf], colors=['blue', 'red'], alpha=0.3)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-2_2xnQ_lksV"},"source":["## Mean Squared Error (MSE) Loss Function\n","Define a MSE function to evaluate how good/bad the model was."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":225,"status":"ok","timestamp":1694659689782,"user":{"displayName":"Lin Zhang","userId":"02863611533796321015"},"user_tz":300},"id":"DoaZpBRqlksW","outputId":"3e25cf49-00c6-4ec6-aa4e-31d342072b71"},"outputs":[],"source":["def mse_loss(pred, label):\n","    \"\"\" Mean Square Error function\n","    Args:\n","        prediction: column vector of predictions, 2d-array with shape (# samples, 1)\n","        label: column vector of ground truths, 2d-array with shape (# samples, 1)\n","    Returns:\n","        loss_value: scalar\n","    \"\"\"\n","    loss_value = np.mean(0.5 * (pred - label) ** 2)\n","    return loss_value\n","\n","# Sanity check\n","loss = mse_loss(pred=linear(X_train, w_dummy, b_dummy), label=y_train)\n","print(f\"mse loss: {loss}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Gradient Descent Optimization"]},{"cell_type":"markdown","metadata":{},"source":["### Gradient Descent Iterations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define gradient computation function\n","def grad(feature, pred, label):\n","    \"\"\" Gradient function\n","    Args:\n","        feature: feature matrix, 2d-array with shape (# samples, # features)\n","        pred: column vector of predictions, 2d-array with shape (# samples, 1)\n","        label: column vector of ground truths, 2d-array with shape (# samples, 1)\n","    Returns:\n","        dw: row vector of MSE loss partial derivatives w.r.t. weights, 2d-array with shape (1, # features)\n","        db: scalar of MSE loss partial derivatives w.r.t. bias\n","    \"\"\"\n","    dw = 1 / label.shape[0] * ((pred - label).T @ feature)  # dL/dw\n","    db = (pred - label).mean()  # dL/db\n","    \n","    return dw, db\n","\n","# Init parameters\n","w = np.random.normal(0, 1e-4, (1, 2)) \n","b = np.random.normal(0, 1e-4)\n","print(f\"initial parameters: w = {w}, b = {b}\")\n","# Gradient descent optimization\n","num_iters = 500\n","learning_rate = 0.05\n","losses = []\n","for i in range(num_iters):\n","    y_pred = linear(X_train, w, b)\n","    dw, db = grad(X_train, y_pred, y_train)\n","    loss = mse_loss(y_pred, y_train)\n","    w = w - learning_rate * dw\n","    b = b - learning_rate * db\n","    print(f\"loss @ {i+1} iteration: {loss}\")\n","    losses.append(loss)\n","print(f\"final parameters: w = {w}, b = {b}\")\n","\n","# Observe the loss change\n","plt.plot(losses)"]},{"cell_type":"markdown","metadata":{},"source":["### Visualize Decision Boundary\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Credit to ChatGPT\n","# Create a meshgrid of points over the feature space\n","xx, yy = np.meshgrid(np.linspace(-1.1, 1.1, 200),\n","                     np.linspace(-1.1, 1.1, 200))\n","zz = w[0, 0] * xx + w[0, 1] * yy + b - 0.5\n","\n","\n","bound_x = np.linspace(-1.1, 1.1)\n","bound_y = (0.5 - b - w[0, 0] * bound_x) / w[0, 1]\n","plt.scatter(X_train[:, 0], X_train[:, 1], s=5*np.ones(y_train.size), c=y_train, cmap='coolwarm')\n","plt.plot(bound_x, bound_y, 'k')\n","plt.contourf(xx, yy, zz, levels=[-np.inf, 0, np.inf], colors=['blue', 'red'], alpha=0.3)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Sigmoid Activation\n","A sigmoid function: $f(x) = 1 / (1 + e^{-x})$ will always output values within [0, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define ReLU function\n","def sigmoid(x):\n","    \"\"\" Sigmoid function\n","    Args:\n","        x: independent variable, could be an arrary of any shape or a scalar.\n","    Returns:\n","        y: dependent variable, could be an arrary of any shape or a scalar.\n","    \"\"\"\n","    y = 1 / (1 + np.exp(-x))\n","    return y\n","\n","# Redefine forward pass. Intermediate result, Z, needs to be tracked \n","def forward(feature, weight, bias):\n","    \"\"\" Model function\n","    Args:\n","        input: feature matrix (independent variables), 2d-array with shape (# samples, # features)\n","        weight: row vector of weights, 2d-array with shape (1, # features)\n","        bias: scalar\n","    Returns:\n","        output: column vector of predictions (sigmoid activated outcomes), 2d-array with shape (# samples, 1)\n","        Z: column vector of intermediate outputs, 2d-array with shape (# samples, 1)\n","    \"\"\"\n","    Z = linear(feature, weight, bias)\n","    pred = sigmoid(Z)\n","    return pred\n","\n","# Redefine gradient function. An exatra step to calculate dL/dZ will be added. \n","def grad(feature, pred, label):\n","    \"\"\" Gradient function with sigmoid activation\n","    Args:\n","        feature: feature matrix, 2d-array with shape (# samples, # features)\n","        pred: column vector of predictions, 2d-array with shape (# samples, 1)\n","        label: column vector of ground truths, 2d-array with shape (# samples, 1)\n","        Z: column vector of intermediate outputs, 2d-array with shape (# samples, 1)\n","    Returns:\n","        dw: row vector of MSE loss partial derivatives w.r.t. weights, 2d-array with shape (1, # features)\n","        db: scalar of MSE loss partial derivatives w.r.t. bias\n","    \"\"\"\n","    dZ = (pred - label) * pred * (1 - pred)\n","    dw = 1 / label.shape[0] * (dZ.T @ feature)\n","    db = dZ.mean()\n","    \n","    return dw, db\n","\n","# Optimization iterations\n","w = np.random.normal(0, 1e-4, (1, 2)) \n","b = np.random.normal(0, 1e-4)\n","print(f\"initial parameters: w = {w}, b = {b}\")\n","num_iters = 1000\n","learning_rate = 0.9\n","losses = []\n","for i in range(num_iters):\n","    y_pred = forward(X_train, w, b)\n","    dw, db = grad(X_train, y_pred, y_train)\n","    loss = mse_loss(y_pred, y_train)\n","    w = w - learning_rate * dw\n","    b = b - learning_rate * db\n","    print(f\"loss @ {i+1} iteration: {loss}\")\n","    # print(f\"w = {w}, b = {b}\")\n","    losses.append(loss)\n","print(f\"final parameters: w = {w}, b = {b}\")\n","\n","# Observe loss values\n","plt.plot(losses)"]},{"cell_type":"markdown","metadata":{},"source":["### Visualize Sigmoid Function Activated Model Decision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Credit to ChatGPT\n","# # Create a meshgrid of points over the feature space\n","# xx, yy = np.meshgrid(np.linspace(-1.1, 1.1, 200),\n","#                      np.linspace(-1.1, 1.1, 200))\n","# zz = sigmoid(w[0, 0] * xx + w[0, 1] * yy + b) - 0.5\n","\n","\n","# # bound_x = np.linspace(-1.1, 1.1)\n","# # bound_y = (0.5 - b - w[0, 0] * bound_x) / w[0, 1]\n","# plt.scatter(X_train[:, 0], X_train[:, 1], s=5*np.ones(y_train.size), c=y_train, cmap='coolwarm')\n","# # plt.plot(bound_x, bound_y, 'k')\n","# plt.contourf(xx, yy, zz, levels=[-np.inf, 0, np.inf], colors=['blue', 'red'], alpha=0.3)\n","\n","\n","# Create a meshgrid of points over the feature space\n","xx, yy = np.meshgrid(np.linspace(-1.1, 1.1, 200),\n","                     np.linspace(-1.1, 1.1, 200))\n","zz = sigmoid(w[0, 0] * xx + w[0, 1] * yy + b) - 0.5\n","\n","\n","db_x = np.linspace(-1.1, 1.1)\n","db_y = (-b - w[0, 0] * db_x) / w[0, 1]\n","plt.scatter(X_train[:, 0], X_train[:, 1], s=5*np.ones(y_train.size), c=y_train, cmap='coolwarm')\n","plt.plot(db_x, db_y, 'k')\n","plt.contourf(xx, yy, zz, levels=[-np.inf, 0, np.inf], colors=['blue', 'red'], alpha=0.3)\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## "]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"3321","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
